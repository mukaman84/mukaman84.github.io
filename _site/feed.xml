<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dongyul Lee</title>
    <description>Welcome to Dongyul DNN space</description>
    <link>https://mukaman84.github.io/</link>
    <atom:link href="https://mukaman84.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 27 Mar 2019 00:50:33 +0900</pubDate>
    <lastBuildDate>Wed, 27 Mar 2019 00:50:33 +0900</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Portfolio</title>
        <description>&lt;h2 id=&quot;face-recognition-and-image-processing&quot;&gt;Face recognition and image processing&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Depth estimation with stereo camera&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 26 Mar 2019 00:00:00 +0900</pubDate>
        <link>https://mukaman84.github.io/2019/03/26/Resume.html</link>
        <guid isPermaLink="true">https://mukaman84.github.io/2019/03/26/Resume.html</guid>
        
        
      </item>
    
      <item>
        <title>얼굴인식과 영상인식 처리</title>
        <description>&lt;h2 id=&quot;face-recognition-and-image-processing&quot;&gt;Face recognition and image processing&lt;/h2&gt;

&lt;p&gt;1강- 얼굴인식, 영상인식 및 처리 산업현황과 국내외 주요 업체 사업 동향&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;라온피플 이석중 대표&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(1) rule 기반 비전 검사 및 육안 검사의 한계
 -왜 딥러닝이 적용되었는지 살펴보자&lt;/p&gt;

&lt;p&gt;기존 머신비전은 조명등의 환경을 맞추어 제한된 환경에서만 동작하였다.
기존 비전검사 방법은 어려웠다 이유는
    1) 이미지 획득, 전처리
    2) 중간과정에서는 세그멘테이션, 재해석 등 
    3) 해석&lt;/p&gt;

&lt;p&gt;각 과정별로 다양한 툴 언어가 존재한다.&lt;/p&gt;

&lt;p&gt;비전 검사시 주요 어려운점은 - 각 제품별 불량의 기준이 다를수있다.&lt;/p&gt;

&lt;p&gt;다시 말하면, 정상제품하고 모습이 달라도 불량이 아닌 정상일 수 있다.&lt;/p&gt;

&lt;p&gt;-&amp;gt; 이러한 어려움은 케이스 바이 케이스로 정의가 어려워, 육안검사 + 머신 비전 검사를 섞어서 사용하였다.&lt;/p&gt;

&lt;p&gt;단점&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;이 경우 작업자의 수준에 따라 다양한 편차가 발생한다.&lt;/li&gt;
  &lt;li&gt;단순히 육안검사를 위한 인건비 증가 뿐만이 아니라 검사를 위한 인건비 증가등의 비용이 발생한다.&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;비전검사 종류&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;기존 비전검사&lt;/li&gt;
  &lt;li&gt;-&amp;gt; 육안검사&lt;/li&gt;
  &lt;li&gt;-&amp;gt; 딥러닝 비전검사&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;프루닝 사용이유&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;gpu의 비싼 파워 떄문
요새는 자동화지만 fixed-point approximation의 논문을 보면&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;hardware-oriented approximatino of convolutional neural networks (2016)
convolution에는 MAC관련 리소스가 엄청 잡아먹는다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;최적화하는 즉, 딥러닝 가속기 만드는 사람들은 컨볼루션을 가속화하는 작업을 수행한다.
먼저 fpga로 가속화하는 엔진을 만들고 soc를 양산&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;자연 영상과 산업 영상의 특성 차이&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;자연: 전 영역에서 색상이 넓게 분포&lt;/li&gt;
  &lt;li&gt;산업: 특정 영역의 색상만이 일정하게 나옴&lt;/li&gt;
  &lt;li&gt;대체적으로 align되어있음&lt;/li&gt;
  &lt;li&gt;이미지 사이즈가 비교적 큼&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;-&amp;gt; 어떤 방법 사용?&lt;/p&gt;

&lt;p&gt;=&amp;gt; 설계 목표 
1) 오픈 소스 대비 파라미터 수 1/100 이하로 감소
2) 대용량 이미지에 대한 실시간 inference가 가능해야한다&lt;/p&gt;

&lt;p&gt;여러대의 카메라를 사용했을때 큰 크기의 이미지가 존재
여러각도에서 이미지 촬영을 한 경우 존재&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;여러각도에서 촬영하면서 상태를 classifier&lt;/li&gt;
  &lt;li&gt;멀티 조명을 사용하여 상태를 classifier
 =&amp;gt; 이 경우 멀티 채널 이미지로 만들어 네트워크를 돌리면 효율적이었다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ㄱ기존 네트워크 두고 새로운 네트워크 제안하는 방식으로 신규 불량, 클래스에 대한 결정을 시행&lt;/p&gt;

&lt;p&gt;2강- AI 영상 인식 플랫폼 개발과 응용/상용화 동향&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;IoT 기반에서 현대인의 생활 환경의 변화&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;google glasses 등 다양한 서비스가 존재&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;deep learning with knowledge
=&amp;gt; semantic map -&amp;gt; 아주 중요
비젼 정보와 knowledge를 결합하는 시스템
sensetime의 경우 국가 비전 감시시스템과 유사&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;context-aware service - gps, iot, sensors, automated reasoning and visualization&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;layers of AR Service with IoT&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AR Techonology Support&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Technology Maintenance&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training with VRA/AR Virtual Experience&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;3강 - 컴퓨터비전 기반 AI 영상인식 및 처리 방식별 으용 기술개발과 서비스 동향&lt;/p&gt;

&lt;p&gt;이력서에 docker, agile process관련 기술 설명할 것&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;monocular depth&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;superdepth
-&amp;gt; super resoultion으로 영상크기 확장하고 이를 이용하여 depth 추정
-&amp;gt; 3D map construction과 mapping까지 수행함&lt;/p&gt;

&lt;p&gt;noise2noise
resolution에 대해서 psnr을 추출하고 psnr이 최소화되는 방향으로 네트워크 돌려서 노이즈 최소화&lt;/p&gt;

&lt;p&gt;gaugan
2D-&amp;gt;3D 변경&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;중요 포인트 : 배치 노말라이제이션
=&amp;gt; data 수집에 한계가 있어 이렇게 만드는 것이 빠를수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;video2video-synthesize&quot;&gt;video2video synthesize&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;frame과 frame사이의 영상을복원&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;xnor.ai
저가형 하드웨어 업체&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;fpga 욜로 알고리즘 동작&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;binarization으로 convolution 필터를 구현하였고, 이를 저가형 fpga로 컨버팅
차세대 아키텍처 이용&lt;/p&gt;

&lt;p&gt;non-local block -&amp;gt; RNN 사용하지도 않고 시퀀스 데이터에서 성능을 향상 시킬 수 있음
(어디서 한겨)&lt;/p&gt;

&lt;p&gt;CRF 사용 요령
-&amp;gt; 전처리로 deblur(어떤 deblur 방식이 좋은가?), color normalization
-&amp;gt; 후처리로는 CRF가 너무 거대하기 때문에 부분적으로 사용&lt;/p&gt;

&lt;p&gt;4강 - AI 영상인식 얼굴인식 처리 최적 알고리즘
영상인식 동향&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;jetson TK1 -&amp;gt; 케플러 시리즈 등등&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;임베디드로 카페2, tensorRT, tensorflowLITE - android와 iOS전용&lt;/p&gt;

&lt;p&gt;모델 압축 예시 4가지
-1. parameter pruning and weight sharing&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;low-rank factorization&lt;/li&gt;
  &lt;li&gt;Transferred/compact convolutional filters&lt;/li&gt;
  &lt;li&gt;Teachers-student model&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;1) pruning&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;uncritical인것들을 제거하는 것&lt;/li&gt;
  &lt;li&gt;처음에 전부다 트레이닝하고. 이후 가지치기를 해서 임계값보다 큰 것들을 살려두는 식으로 반복 수행&lt;/li&gt;
  &lt;li&gt;quantization도 이 이안에 포함됨
-&amp;gt; weight sharing으로 비슷한 애들끼리 클러스터링 후 센트로이드를 구하여, 모든 웨이트들이 이 값만을 사용하는 것&lt;/li&gt;
  &lt;li&gt;최근 방법은 CVPR2018에서 발표된 Pruning-Quantization 기술로 압축하는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2) Low-rank factorization&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;convolution을 전체를 하는 것이 아닌 depthwise convolution 및 point-wise convolution을 적용하는 것&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;5강 - 얼굴인식 프레임워크&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;기존 기술은 : 정면 영상을 촬영하는 스태틱 방식이었으면
최근 동향은 비디오에서 얼굴인식을 하는 기술이 연구되고 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;sensetime, megvil;l,&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;dl&gt;
  &lt;dt&gt;1) 얼굴인식 파이프라인&lt;/dt&gt;
  &lt;dd&gt;입력 영상이 들어오면 얼굴을 추출
-&amp;gt; nowledge로부터 얼굴 수평방향 조절 등) -&amp;gt;. 특징 추출 -&amp;gt; 매칭 분류&lt;/dd&gt;
&lt;/dl&gt;

&lt;ul&gt;
  &lt;li&gt;dataset : FDDB, wide face&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;얼굴인식 기술
1) Verification,
   gallery based,
   classification&lt;/p&gt;

&lt;p&gt;Mega face dataset
FaceNet -&lt;/p&gt;

&lt;p&gt;6강 - continual learning
=&amp;gt; Dynamic하게 계속 하는 방법 -&amp;gt; 일부 뉴론만 학습 또는 네트워크 확장 개념&lt;/p&gt;

&lt;p&gt;=&amp;gt; Unsupervised Domain Adaptation
-&amp;gt;&lt;/p&gt;

&lt;p&gt;7강 - 모바일 카메라 기반 기술 설명&lt;/p&gt;

&lt;p&gt;=&amp;gt; outfcusing -&amp;gt; 다양한 화질 개선
 -&amp;gt; 거리갑추정에서 먼거리 값을 blur&lt;/p&gt;

&lt;p&gt;slam -&amp;gt; 결국 다차원 공간 정보 필요
-&amp;gt; 결국 depth 추출이 중요하다.&lt;/p&gt;

&lt;p&gt;필요 코어 기술
1) 이미지 프로세싱 및 전처리
2) 이미지 베이스 퍼셉션
3) 나의 위치 및 포즈 예측&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;주요 설명내용
=&amp;gt; Depth map estimation
 -&amp;gt; 현재 제일 좋은 기술은 stereo Matching Network
   -&amp;gt; Pyramid Stereo Matching Network : feature를 모래시계로 쌓는것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;stereo camera는 구현문제가 있어 요새 유행하는 방식은 depth-estimation을 single camera로 하는게 유행&lt;/p&gt;

&lt;p&gt;unsupervised monocular depth estimation with left-right coninsistency - cvpr 2017&lt;/p&gt;

&lt;p&gt;다초점 카메라 - 카메라 자체로 독특한 multi-array camera를 쓰면 all-focusing 카메라를 얻을 수 있음
-&amp;gt; 거리정보도 다양한 값을 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;즉 모바일의 카메라 기종에 따라 다양한 기술 개발 가능&lt;/p&gt;

&lt;p&gt;예로 베이어 패턴(RGBG)의 카메라가 아닌 RGB(IR) 카메라를 쓰면 IR부분이 apperture(Exposure)부분이 짧기 때문에 더 많은 영역을 볼수가 있다.&lt;/p&gt;

&lt;p&gt;dual pixel phone의 경우 두개의 픽셀 정보를 얻을 수 있기 때문에 하나의 카메라로 거리정보를 얻을 수 있다.&lt;/p&gt;
</description>
        <pubDate>Tue, 26 Mar 2019 00:00:00 +0900</pubDate>
        <link>https://mukaman84.github.io/2019/03/26/Computer-vision-face-recognition.html</link>
        <guid isPermaLink="true">https://mukaman84.github.io/2019/03/26/Computer-vision-face-recognition.html</guid>
        
        
      </item>
    
      <item>
        <title>Pr Byesian Nn</title>
        <description>&lt;p&gt;본 포스트는 다음의 post를 간단히 요약한 내용입니다.&lt;/p&gt;

&lt;p&gt;https://medium.com/@joeDiHare/deep-bayesian-neural-networks-952763a9537&lt;/p&gt;

&lt;p&gt;글쓴이는 neural network의 불확실성을 모델링하기 위해 3개의 bayesian approach를 제안한다.&lt;/p&gt;

&lt;p&gt;1) MCMC integral의 근사화
2) black-box variational inference
3) MC dropout사용&lt;/p&gt;

&lt;p&gt;기존의 전통적인 방식은 likekihood 최대로하는 optimal value를 학습하는, 즉 weight와 bias를 예측하는 방법이었다. 이 경우 weight와 bias는 scalars 값을 가진다. 반면에, Bayesian approach는 이 파라미터들의 분포에 관한 것이다.&lt;/p&gt;

&lt;p&gt;예를들어 앞선 weight와 bias는 학습이 완료된 네트워크가 가질수 있는 값드의 분포로 표현될 수 있다.
나의 해석으로는 네트워크가 학습될 수 있는 local minima가 엄청나게 많은데 그 값들이 분포를 가짐을 의미한다고 생각한다.&lt;/p&gt;

&lt;p&gt;그렇다면 여기서 생각해볼 내용은, single value가 아닌 분포를 가진다는 것이 어떠한 장점이 있을까? 글쓴이는 이를 당연하게 분포의 관점에서 설명한다.
즉, 분포의 의미인 샘플링을 거듭하여 학습하였을 경우에 지속적으로 같은 예측을 한다면 그 결과는 신뢰할만 하다는 내용이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*zCgkD5l7Tyzrch13ndWOfg.png&quot; alt=&quot;by_curve&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다시 분포로 돌아가면, 문제는 deep neural network란 다음의 posterior pdf를 찾는 문제로 볼 수 있다. 여기서 posterior는 당연하게도 우리가 아는 Bayes rule을 통해서 얻어진다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*zMq4VYW4P-OVkOH_IgFF4A.gif&quot; alt=&quot;by_pdf&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 식에서 가장 문제는 당연히 분모 P(x,y) 이다. 왜냐하면 수식에서 처럼, 모든 weight 파라미터에 대해서 이 확률을 다 구하는 것은 불가능에 가깝기 때문이다. 결국 exhaustive search인데 가능하겠는가?&lt;/p&gt;

&lt;p&gt;결국 Deep Bayesian Neural Network 방식은 이 분모를 근사화시켜서 푸는 방식을 이야기하는 것이다.&lt;/p&gt;

&lt;p&gt;먼저 방안 1은 Markov Chain MonteCarlo를 사용하는 것이다. 사실 나는 Markov chain과 MonteCarlo 각각은 당연히 엄청나게 많이 들어봤지만, 이 둘을 결합한 방식은 처음 들어보았다. 하지만 내가 짧게 공부하기론 그냥 그 둘은 섞은 방식으로 이해했다. 즉 Markov chain은 state와 trainsiotion이 정의 되고 이들을 확률로 표현한 모델을 이야기하는데, 이렇게 표현된 모델을 MonteCarlo 방식으로 확률을 구한 것이다. 좀 더 딥러닝 관점에서 생각해 보자면, 네트워크거 여러 계층으로 구성되는 상황에서 각 계층을 state로 표현하고 state간 이동인 weight를 trainsition으로 표현하는 것이라고 이해하면 될 듯하다. 즉, posterior probability를 DnC하여 푸는 방식이라고 생각하면 편할 듯하다.&lt;/p&gt;

&lt;p&gt;다음으로 글쓴이가 설명한 Byesian NN을 근사화하는 2번째 방법은 black-box variation inference이다.&lt;/p&gt;

&lt;p&gt;먼저 variation inference 정의를 살펴보면&lt;/p&gt;

&lt;p&gt;Variational inference is an approach to estimate a density function by choosing a distribution we know (eg. Gaussian) and progressively changing its parameters until it looks like the one we want to compute, the posterior. 
Variational inference is an approach to estimate a density function by choosing a distribution we know (eg. Gaussian) and progressively changing its parameters until it looks like the one we want to compute, the posterior.&lt;/p&gt;

</description>
        <pubDate>Sun, 17 Mar 2019 00:00:00 +0900</pubDate>
        <link>https://mukaman84.github.io/2019/03/17/PR-Byesian-NN.html</link>
        <guid isPermaLink="true">https://mukaman84.github.io/2019/03/17/PR-Byesian-NN.html</guid>
        
        
      </item>
    
      <item>
        <title>GPU architecture and CUDA programming</title>
        <description>&lt;p&gt;본 페이지에서는 “ 머신러닝과 블록체인을 떠받치는 GPU의 모든기술”이라는  도서를 Deep Dive 하여 모두 파헤치기 
위해 작성하였다.&lt;/p&gt;

&lt;p&gt;먼저 GPU의 주된 처리 방식에 대해서 정리해본다.&lt;/p&gt;

&lt;p&gt;저자의 책  그림 1.13 보면 알 수 있듯이, GPU와 CPU는 일반적으로 DMA(Direct Memory Access)엔진 을 
사용하여  양쪽 메모리 사이의  데이터를 전송하였다. ( 물리적 연결은 어떻게? – DMA에 대하여 대하여 보다 ㄱ오부할 것)&lt;/p&gt;

&lt;p&gt;메모리 공유방법 :
1) DMA
2) PCI Express를 경유 -&amp;gt; 통신 오버헤드가 큼(gpu -&amp;gt;PCI -&amp;gt;보드-&amp;gt;CPU 등등)&lt;/p&gt;

&lt;p&gt;이 방식들의 문제는 무엇인가?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;GPU와 CPU간에 별도 메모리가 존재하면 깊은 복사로 인해 오버헤드가 상당하다. 따라서 공통의 메모리 공간을 갖추는 것이 중요하나 
다음의 한계로 어렵다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1) CPU - 대용량 메모리 필요
2) GPU - 고연산 성능을 가능하기 위한 고 대역폭 메모리 필요&lt;/p&gt;

&lt;p&gt;-&amp;gt; 즉 gpu 제조사는 이러한 한계속에서 공통 메모리 공간을 다루는 것이 중요한 개발 목표가 됨 -&amp;gt; cpu 제조사도 마찬가지인듯?
예로 CPU 제조사에서 고 대역폭 요구사항을 맞춰 주기 위하여 일반적으로 cpu에서 사용되는 DDR3/4 메모리 경우 대역폭이 부족하여 GDDR DRAM등의 방법으로 메모리 대역폭을 개선중에 있다. 또한 L4 캐시를 장착하여 GPU가 요구하는 높은 메모리 대역을 실현하고 있다.zz&lt;/p&gt;

&lt;p&gt;CUDNN은 딥러닝을 위해서 반복적으로 사용되는 컨볼루션, 폴링, signoid, Batchnorm(?)과 같은 기본적인 기능들을 정해진 알고리즘에 따라서 cuda 프로그래밍으로 사전에 구현해 놓은 라이브러리를 가리킨다. 예로 컨볼루션의 경우 gemm, fast gemm, FFT, Winograd 알고리즘이 대표적인 것이다.
아래의 그림은 gemm을 CUDA 프로그래밍으로 구현하는 기본 메카니즘을 보여준다.
&lt;img src=&quot;https://www.groundai.com/media/arxiv_projects/11346/genr.svg&quot; alt=&quot;CuDNN&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Mar 2019 00:00:00 +0900</pubDate>
        <link>https://mukaman84.github.io/2019/03/09/tf_record_api.html</link>
        <guid isPermaLink="true">https://mukaman84.github.io/2019/03/09/tf_record_api.html</guid>
        
        
      </item>
    
      <item>
        <title>Bayesian Neural Net Introduction</title>
        <description>&lt;h2 id=&quot;baysian-nerural-networks&quot;&gt;Baysian Nerural Networks&lt;/h2&gt;

&lt;p&gt;본 포스트에서는 Baysian Nerural Networks에 대해서 기초부터 최신 논문 동향까지를 기술한다.&lt;/p&gt;

&lt;p&gt;먼저 기본적인 설명은 &lt;a href=&quot;https://medium.com/neuralspace/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2&quot;&gt;Bayesian Neural Network&lt;/a&gt; post를 참고하여 재구성하였다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/neuralspace/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2&quot;&gt;Bayesian Neural Network&lt;/a&gt; postsms 8개의 post를 통해서 설명하고 있다. 이 중 본 post는 첫 번째인 Need for Bayesian Neural Networks &lt;a href=&quot;https://medium.com/neuralspace/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2&quot;&gt;Bayesian Neural Network&lt;/a&gt;를 주로 참조하였다.&lt;/p&gt;

&lt;p&gt;Baysian Nerural Networks를 이해하기 위해 먼저 point-estimate에 대해서 이해할 필요가 있다.
Point-estimate : 아래 그림의 왼쪽 네트워크 예시와 같이 weight(filter의 각 element)의 가 sinle point로 표현되는 것을 의미한다.
Baysian Nerural Network: Point-estimate와는 다르게 아래의 그림에서 오른쪽 네트워크와 같이 weight가 확률의 형태로 표기됨을 의미한다.&lt;/p&gt;

&lt;p&gt;그럼 Point-estimate는 왜 문제인가? Over-fitting 때문&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;결국 최종단에서 softmax가 각 pixel 또는, 이미지들의 class를 결정하는데 있어, 하나의 class외에 나머지를 squish하여 zero 근처로 보내버리고 오직 하나만을 maximize하는 방향으로 가버리는데 이는 one class에 대한 overconfident 결정일 수 있다. 특히 imbalanced dataset에서 이러한 overconfident 결정은 더 두드러지게 나타난다.
 즉, 동물을 classfication하는 문제라고하면 개일 확률 0.9로 만들고 나머지는 0.1 이하로 만드는 결정을 할 수 있다.
 때로는 강아지 0.4 늑대 0.3 고양이 0.2로 inference하는 방식이 네트워크의 overfit를 방지할 수 있다.
 -&amp;gt; 그럼 뒤에 결정은 어떻게 하는가? -&amp;gt; 좀 더 고민해 보자&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*n6Td0BSmvCGaTYaIJEqF-g.png&quot; alt=&quot;Bayesian_Net&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그런데 Overfitting or making overconfident decisions문제를 풀기위해 많은 regularization techniques들이 존재한다.
이들과 Baysian Nerural Network의 차이는 무엇인가? -&amp;gt; 기존 regularization techniques은 결정된 정보의 불확실함을 표현하지 못한다.
그러나 Baysian Nerural Network는 결정된 부분에서 불확실함이 네트워크에 표현이 가능하다.&lt;/p&gt;

&lt;h2 id=&quot;the-practicality-of-bayesian-neural-networks&quot;&gt;The practicality of Bayesian neural networks&lt;/h2&gt;
&lt;p&gt;이 파트는 Bayesian neural networks를 어떻게 구현할 것인가에 대한 얘기로, 네트워크의 여러 파라미터 (weights, activation results등) 중에서 무엇을 확률 모델로 바꿀것 인가를 다루는 문제이다. 가장 쉽게 생각할 수 있는건 weights를 확률 모델로 가져가는 것이다. 그러나 여태까지 그 누구도 weights를 distribution 모델로 가져가서 성공한 사례는 없다. 그 이유는 너무나 많은 weight 수와 모델의 크기 때문이 아닐까라고 예측되고있다.&lt;a href=&quot;https://medium.com/neuralspace/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2&quot;&gt;1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;그러한 이유로 Bayesian neural networks 연구를 시간의 순서대로 살펴볼 필요가 있다. 초기 연구는 가능성을 보기 위해 FC 구조의 neural network에서 model posterior에 대해 근사화하는 것을 시작으로 한다. 초기에는 posterior 확률로 가우시안 분포와 같은 simple variational distribution을 사용하였고, 네트워크가 학습을 통해서 이러한 true posterior probability에 가까운 분포를 만들어 내도록 학습되었다. 이를 위해 확률 분포간의 가격을 최소화하는 KL-divergence가 사용되었다.&lt;/p&gt;

&lt;p&gt;그러나 이 연구의 문제는 gaussian 근사화로 인해 model parameter들의 수 증가가 너무나 커져서 계산적으로 비싸다는 것이다. 예로 가우시안 근사화에 사용된 파라미터만으로 모델 파라미터들의 두배가 된다는 것이다.&lt;/p&gt;

&lt;p&gt;-» 검토 필요 : 그런데 dropout을 사용한 전통적인 방식이 동일한 predictive performance를 보여준다는 결과가 나왔다???&lt;/p&gt;

&lt;p&gt;이는 모델의 모든 파라미터들은 변수가 될 수 있고, 이 각각의 변수들을 bayesian 확률 모델로 정의하는 것은 엄청난 계산 복잡도를 가져온다는 것이다. 그래서 Bayesian neural networks 모델을 만드는 수 많은 방법들이 존재하는데 이 포스트에서는 backpropagation에 bayes 이론을 적용하는 것을 집중적으로 살펴본다.&lt;/p&gt;

&lt;h2 id=&quot;bayes-by-backprop&quot;&gt;Bayes by Backprop&lt;/h2&gt;

&lt;p&gt;Bayes by backprop는 &lt;a href=&quot;https://arxiv.org/abs/1505.05424&quot;&gt;Blundell, et al.&lt;/a&gt;에 의해서 처음 도입되었다. 이 논문에서는 gradient의 unbiased Monte Carlo estimates를 사용한 앙상블들로 네트워크의 파라미터들을 학습시켰다.&lt;/p&gt;

&lt;p&gt;그런데 네트워크 파라미터가 점점 증가함에 따라 정확하게 학습시키는 것은 불가능하였다. 
이에 대한 해결책으로 제시된 방법은 최근에 사용되는 variational approximation 기술이다.
기존 Monte Carlo 기술은 너무나 많은 앙상블이 요구되기에 Bayesian posterior distribution을 approximate시키는 것은 정말 어렵기 때문이다.&lt;/p&gt;

&lt;p&gt;왜 학습이 어려운지는 &lt;a href=&quot;https://medium.com/neuralspace/bayesian-neural-network-series-post-2-background-knowledge-fdec6ac62d43&quot;&gt;Bayesian neural networks&lt;/a&gt; 2번 째 blog post에 다음과 같이 잘 설명되어있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bayes theorem은 다음과 같다.
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*7iOrI5jb6Dae630hCYENjA.png&quot; alt=&quot;Bayes theorem&quot; /&gt;
일반적으로 supervised NN은 data x가 주어질 경우 model parameters θ를 찾는 문제로, 이로부터 posterior probability P(θ|x)가 정의된다.
여기서 P(θ) 는 prior이며, P(x|θ) which is the likelihood로 data distribution을 갖는다. 
여기서 P(x)는 evidence로 모델로 부터 생성된다. 즉 다음과 같이 얻어진다.
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*FzrX_7Qb7m1n6eXO2zrE9Q.png&quot; alt=&quot;p_x_evidence&quot; /&gt; 
여기서 알 수 있듯이 p(x)를 얻기위해서는 모든 모델 파라미터와 x가 결합되었을떄 발생하는 모든 경우의수를 얻어야하는데, 이는 모델 파라미터의 엄청난 수로 인해 불가능하다. 따라서 이를 approximate하는 방식이 필요한데, 요새는 variational inference라는 모델이 사용된다.
이 방법외에 (Markov Chain Monte Carlo and Monte Carlo Dropout.)방식도 사용된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variational-inference&quot;&gt;Variational Inference&lt;/h2&gt;
&lt;p&gt;variation inference 방식으로 풀기 위해 각 probabilty는 density function을 갖으며, 이를 예측하는 문제라고 가정한다. 따라서 알려진 distribution중에 하나를 target density function으로 가정하는 것으로부터 시작한다. 이로부터 우리가 찾는 문제는 posterior probability와 true distrbition간의 거리를 최대한 가깝게 만드는 것을 목표로한다. 이를 위해서 사용되는 기술이 the Kullback-Liebler (KL) divergence이다.&lt;/p&gt;

&lt;p&gt;예로 모델의 웨이트 파라미터를 w, Data를 D라하면 true posterior probability를 P(w|D)라 할수 있으며, 다른 distribution은 q(w|D)라 할 수 있다.
이때 KL divergence는 다음과 같이 정의된다.
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*b08FgIvbikjpX0ZTraY1sg.png&quot; alt=&quot;KL_divergence&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 최적화 문제를 풀어보면 다음과 같이 전개되는데 ingegral function으로 인해 사실상 푸는 것이 불가능하다.
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*sZGFVuHKPZdhROYNWEy9YQ.png&quot; alt=&quot;KL_divergence2&quot; /&gt;
식에서 보면 알 수 있듯이 true posterior function p(w|D)는 사실상 다루기 어렵기 때문에 q(W|D)를 근사화하여 문제를 푸는 것이 훨씬 용이하다.&lt;/p&gt;

&lt;p&gt;이에 다음과 같이 모델을 근사화 할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*88qCMa1S_2v-dWSbtwEG_A.png&quot; alt=&quot;KL_divergence3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 이 모델은 train이 가능한 형태이다
왜??? -&amp;gt; 이해하기 매우어렵지만 좀더 진행후 이해해보고자 한다.&lt;/p&gt;

&lt;p&gt;그러나 이해하기 앞서 수식에서 보이는 것처럼 weight의 수에 영향을 받기 때문에 weight pruning 기술을 적용하여 networkdml sparsity를 감소시켜 네트워크의 성능을 감소시키지 않는 선에서 모델의 파라미터의 수를 감소시킬 필요가 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*lTZBJeYsohUk7RaFrgg1Jg.png&quot; alt=&quot;Bayesian_inference_total_form&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*n7hGf0h9Q-nwyUex1889Zg.png&quot; alt=&quot;bayes_theorem&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 09 Mar 2019 00:00:00 +0900</pubDate>
        <link>https://mukaman84.github.io/2019/03/09/Bayesian-Neural-Net.html</link>
        <guid isPermaLink="true">https://mukaman84.github.io/2019/03/09/Bayesian-Neural-Net.html</guid>
        
        
      </item>
    
      <item>
        <title>Pr Gcn 01</title>
        <description>&lt;p&gt;Title :&lt;/p&gt;
</description>
        <pubDate>Thu, 27 Dec 2018 00:00:00 +0900</pubDate>
        <link>https://mukaman84.github.io/2018/12/27/PR-GCN-01.html</link>
        <guid isPermaLink="true">https://mukaman84.github.io/2018/12/27/PR-GCN-01.html</guid>
        
        
      </item>
    
      <item>
        <title>Natural Language Processing study</title>
        <description>&lt;p&gt;최근 natural language 공부를 시작하였습니다.&lt;/p&gt;

&lt;p&gt;본 페이지에서는 GPU의 대표 모델별 architecture와 CUDA 프로그래밍&lt;/p&gt;

&lt;p&gt;GPU에 대해서 이해를 위해선 먼저 엔비디아의 주요 제품군을 살펴볼 필요가 있다.
크게 그래픽카드 칩셋인 지포스(GeForce) 시리즈와 컴퓨터 그래픽스 개발를 위해 만들어진
그래픽 카드 칩셋 쿼드로(Quadro) 시리즈, 그리고 딥러닝과 같은 고성능 컴퓨팅용
카드인 테슬라(Tesla) 시리즈가 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://img.danawa.com/images/descFiles/4/520/3519715_1514702757296.png&quot; alt=&quot;GPU 구조1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 중 고성능 컴퓨팅 카드인 테슬라 시리즈 분류의 GPU 구조에 대해서 설명한다. 
위의 그림과 같이 CPU 대비 GUP의 가장 큰 특징은 다수의 ALU로 구성되어 있다는 것인데,
이는 동시에 수많은 ㄴ벡터연산을 하드웨어 수행이 가능하다는것이다. 이러한 특징이 딥러닝 연산에 최적화되어
있는 것이다.
즉, CPU는 ALU 보다는 Controlblock과 cache가 많은 부분 구성되어 있었으나, ALU 연산에는 과도한
SPEC이기에 이러한 요소를 줄이고 ALU 연산에 최적화 시킨 것이다.&lt;/p&gt;

&lt;p&gt;테슬라 시리즈에서 세대별 모델의 순서는 Fermi, kepler, Maxwell, Pascal로 아래의 두 그림과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/J3b8uJn.png&quot; alt=&quot;GPU 구조2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://drmola.com/files/attach/images/56517/096/049/a66371d08d953fa9af604a66c97cbf3e.png&quot; alt=&quot;GPU 구조3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 하드웨어 구조에서 최적의 컴퓨팅 연산을 위해선 ALU 로직과 메모리 접근,fetcing 그리고 재분배 등과 같은
다양한 메모리 제어 방법이 필요하다 이에 따라 이러한 하드웨어와 최적으로 운용하기위한 SW가 필요하며 이러한
기술중 엔디비아가 개발한 대표적인 SW가 CUDA이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/aOKAnQn.png&quot; alt=&quot;GPU 구조4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/CHw8FjP.png&quot; alt=&quot;GPU 구조5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 GPU 구조에서 CUDA 프로그래밍이 하는 역할은 다음 그림들과 같이 순차적으로 발생한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://cfile26.uf.tistory.com/image/221F474C58114C801BED34&quot; alt=&quot;CUDA 1&quot; /&gt;
&lt;img src=&quot;http://cfile27.uf.tistory.com/image/211A474C58114C811E95DB&quot; alt=&quot;CUDA 2&quot; /&gt;
&lt;img src=&quot;http://cfile9.uf.tistory.com/image/2223824C58114C82177403&quot; alt=&quot;CUDA 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CUDNN은 딥러닝을 위해서 반복적으로 사용되는 컨볼루션, 폴링, signoid, Batchnorm(?)과 같은 기본적인 기능들을 정해진 알고리즘에 따라서 cuda 프로그래밍으로 사전에 구현해 놓은 라이브러리를 가리킨다. 예로 컨볼루션의 경우 gemm, fast gemm, FFT, Winograd 알고리즘이 대표적인 것이다.
아래의 그림은 gemm을 CUDA 프로그래밍으로 구현하는 기본 메카니즘을 보여준다.
&lt;img src=&quot;https://www.groundai.com/media/arxiv_projects/11346/genr.svg&quot; alt=&quot;CuDNN&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 16 Dec 2018 00:00:00 +0900</pubDate>
        <link>https://mukaman84.github.io/2018/12/16/NLP_DL_lab.html</link>
        <guid isPermaLink="true">https://mukaman84.github.io/2018/12/16/NLP_DL_lab.html</guid>
        
        
      </item>
    
      <item>
        <title>cs231n review (course 1-4)</title>
        <description>&lt;p&gt;본 페이지는 shell script를 연습하기 위한 페이지로 기억의 끝자락을 따라서 그저 끄적여 본다.&lt;/p&gt;

&lt;p&gt;source config/server.config&lt;/p&gt;

&lt;p&gt;선언되어야할 변수들&lt;/p&gt;

&lt;p&gt;$aa_preprocessor&lt;/p&gt;

&lt;p&gt;$hci_file&lt;/p&gt;

&lt;p&gt;$png_file&lt;/p&gt;

&lt;p&gt;AA preprocessor container 수행&lt;/p&gt;

&lt;p&gt;docker start $aa_preprocessor      –&amp;gt; docker container 시작&lt;/p&gt;

&lt;p&gt;docker exec $aa_preprocessor sh precoess.sh [-i] $hci_file [-o] $png_file
  -&amp;gt; 이안에서 hci2png conversion이 일어남&lt;/p&gt;

&lt;p&gt;각 container 수행&lt;/p&gt;

</description>
        <pubDate>Tue, 15 May 2018 00:00:00 +0900</pubDate>
        <link>https://mukaman84.github.io/2018/05/15/shell-practice.html</link>
        <guid isPermaLink="true">https://mukaman84.github.io/2018/05/15/shell-practice.html</guid>
        
        
      </item>
    
      <item>
        <title>PR review (title: learning to cluster for proposal-free instance segmentation)</title>
        <description>
</description>
        <pubDate>Tue, 15 May 2018 00:00:00 +0900</pubDate>
        <link>https://mukaman84.github.io/2018/05/15/PR-RI-01.html</link>
        <guid isPermaLink="true">https://mukaman84.github.io/2018/05/15/PR-RI-01.html</guid>
        
        
      </item>
    
      <item>
        <title>cs231n review (course 1-4)</title>
        <description>&lt;p&gt;본 페이지에서는 스탠포드 대학의 ccs231n 코스를 지극히 개인관점에서 리뷰 및 핵심내용을 요약하고자 한다.&lt;/p&gt;

&lt;p&gt;먼저 1-4 class의 내용을 다음과 같이 요약한다.&lt;/p&gt;
</description>
        <pubDate>Mon, 14 May 2018 00:00:00 +0900</pubDate>
        <link>https://mukaman84.github.io/2018/05/14/cs231n-review.html</link>
        <guid isPermaLink="true">https://mukaman84.github.io/2018/05/14/cs231n-review.html</guid>
        
        
      </item>
    
  </channel>
</rss>
