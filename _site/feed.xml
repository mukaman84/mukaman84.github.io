<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dongyul Lee</title>
    <description>Welcome to Dongyul DNN space</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 25 Mar 2019 00:37:33 +0900</pubDate>
    <lastBuildDate>Mon, 25 Mar 2019 00:37:33 +0900</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Pr Byesian Nn</title>
        <description>&lt;p&gt;본 포스트는 다음의 post를 간단히 요약한 내용입니다.&lt;/p&gt;

&lt;p&gt;https://medium.com/@joeDiHare/deep-bayesian-neural-networks-952763a9537&lt;/p&gt;

&lt;p&gt;글쓴이는 neural network의 불확실성을 모델링하기 위해 3개의 bayesian approach를 제안한다.&lt;/p&gt;

&lt;p&gt;1) MCMC integral의 근사화
2) black-box variational inference
3) MC dropout사용&lt;/p&gt;

&lt;p&gt;기존의 전통적인 방식은 likekihood 최대로하는 optimal value를 학습하는, 즉 weight와 bias를 예측하는 방법이었다. 이 경우 weight와 bias는 scalars 값을 가진다. 반면에, Bayesian approach는 이 파라미터들의 분포에 관한 것이다.&lt;/p&gt;

&lt;p&gt;예를들어 앞선 weight와 bias는 학습이 완료된 네트워크가 가질수 있는 값드의 분포로 표현될 수 있다.
나의 해석으로는 네트워크가 학습될 수 있는 local minima가 엄청나게 많은데 그 값들이 분포를 가짐을 의미한다고 생각한다.&lt;/p&gt;

&lt;p&gt;그렇다면 여기서 생각해볼 내용은, single value가 아닌 분포를 가진다는 것이 어떠한 장점이 있을까? 글쓴이는 이를 당연하게 분포의 관점에서 설명한다.
즉, 분포의 의미인 샘플링을 거듭하여 학습하였을 경우에 지속적으로 같은 예측을 한다면 그 결과는 신뢰할만 하다는 내용이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*zCgkD5l7Tyzrch13ndWOfg.png&quot; alt=&quot;by_curve&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다시 분포로 돌아가면, 문제는 deep neural network란 다음의 posterior pdf를 찾는 문제로 볼 수 있다. 여기서 posterior는 당연하게도 우리가 아는 Bayes rule을 통해서 얻어진다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*zMq4VYW4P-OVkOH_IgFF4A.gif&quot; alt=&quot;by_pdf&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 식에서 가장 문제는 당연히 분모 P(x,y) 이다. 왜냐하면 수식에서 처럼, 모든 weight 파라미터에 대해서 이 확률을 다 구하는 것은 불가능에 가깝기 때문이다. 결국 exhaustive search인데 가능하겠는가?&lt;/p&gt;

&lt;p&gt;결국 Deep Bayesian Neural Network 방식은 이 분모를 근사화시켜서 푸는 방식을 이야기하는 것이다.&lt;/p&gt;

&lt;p&gt;먼저 방안 1은 Markov Chain MonteCarlo를 사용하는 것이다. 사실 나는 Markov chain과 MonteCarlo 각각은 당연히 엄청나게 많이 들어봤지만, 이 둘을 결합한 방식은 처음 들어보았다. 하지만 내가 짧게 공부하기론 그냥 그 둘은 섞은 방식으로 이해했다. 즉 Markov chain은 state와 trainsiotion이 정의 되고 이들을 확률로 표현한 모델을 이야기하는데, 이렇게 표현된 모델을 MonteCarlo 방식으로 확률을 구한 것이다. 좀 더 딥러닝 관점에서 생각해 보자면, 네트워크거 여러 계층으로 구성되는 상황에서 각 계층을 state로 표현하고 state간 이동인 weight를 trainsition으로 표현하는 것이라고 이해하면 될 듯하다. 즉, posterior probability를 DnC하여 푸는 방식이라고 생각하면 편할 듯하다.&lt;/p&gt;

&lt;p&gt;다음으로 글쓴이가 설명한 Byesian NN을 근사화하는 2번째 방법은 black-box variation inference이다.&lt;/p&gt;

&lt;p&gt;먼저 variation inference 정의를 살펴보면&lt;/p&gt;

&lt;p&gt;Variational inference is an approach to estimate a density function by choosing a distribution we know (eg. Gaussian) and progressively changing its parameters until it looks like the one we want to compute, the posterior. 
Variational inference is an approach to estimate a density function by choosing a distribution we know (eg. Gaussian) and progressively changing its parameters until it looks like the one we want to compute, the posterior.&lt;/p&gt;

</description>
        <pubDate>Sun, 17 Mar 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019/03/17/PR-Byesian-NN.html</link>
        <guid isPermaLink="true">http://localhost:4000/2019/03/17/PR-Byesian-NN.html</guid>
        
        
      </item>
    
      <item>
        <title>GPU architecture and CUDA programming</title>
        <description>&lt;p&gt;본 페이지에서는 “ 머신러닝과 블록체인을 떠받치는 GPU의 모든기술”이라는  도서를 Deep Dive 하여 모두 파헤치기 
위해 작성하였다.&lt;/p&gt;

&lt;p&gt;먼저 GPU의 주된 처리 방식에 대해서 정리해본다.&lt;/p&gt;

&lt;p&gt;저자의 책  그림 1.13 보면 알 수 있듯이, GPU와 CPU는 일반적으로 DMA(Direct Memory Access)엔진 을 
사용하여  양쪽 메모리 사이의  데이터를 전송하였다. ( 물리적 연결은 어떻게? – DMA에 대하여 대하여 보다 ㄱ오부할 것)&lt;/p&gt;

&lt;p&gt;메모리 공유방법 :
1) DMA
2) PCI Express를 경유 -&amp;gt; 통신 오버헤드가 큼(gpu -&amp;gt;PCI -&amp;gt;보드-&amp;gt;CPU 등등)&lt;/p&gt;

&lt;p&gt;이 방식들의 문제는 무엇인가?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;GPU와 CPU간에 별도 메모리가 존재하면 깊은 복사로 인해 오버헤드가 상당하다. 따라서 공통의 메모리 공간을 갖추는 것이 중요하나 
다음의 한계로 어렵다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1) CPU - 대용량 메모리 필요
2) GPU - 고연산 성능을 가능하기 위한 고 대역폭 메모리 필요&lt;/p&gt;

&lt;p&gt;-&amp;gt; 즉 gpu 제조사는 이러한 한계속에서 공통 메모리 공간을 다루는 것이 중요한 개발 목표가 됨 -&amp;gt; cpu 제조사도 마찬가지인듯?
예로 CPU 제조사에서 고 대역폭 요구사항을 맞춰 주기 위하여 일반적으로 cpu에서 사용되는 DDR3/4 메모리 경우 대역폭이 부족하여 GDDR DRAM등의 방법으로 메모리 대역폭을 개선중에 있다. 또한 L4 캐시를 장착하여 GPU가 요구하는 높은 메모리 대역을 실현하고 있다.zz&lt;/p&gt;

&lt;p&gt;CUDNN은 딥러닝을 위해서 반복적으로 사용되는 컨볼루션, 폴링, signoid, Batchnorm(?)과 같은 기본적인 기능들을 정해진 알고리즘에 따라서 cuda 프로그래밍으로 사전에 구현해 놓은 라이브러리를 가리킨다. 예로 컨볼루션의 경우 gemm, fast gemm, FFT, Winograd 알고리즘이 대표적인 것이다.
아래의 그림은 gemm을 CUDA 프로그래밍으로 구현하는 기본 메카니즘을 보여준다.
&lt;img src=&quot;https://www.groundai.com/media/arxiv_projects/11346/genr.svg&quot; alt=&quot;CuDNN&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Mar 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019/03/09/tf_record_api.html</link>
        <guid isPermaLink="true">http://localhost:4000/2019/03/09/tf_record_api.html</guid>
        
        
      </item>
    
      <item>
        <title>Bayesian Neural Net Introduction</title>
        <description>&lt;h2 id=&quot;baysian-nerural-networks&quot;&gt;Baysian Nerural Networks&lt;/h2&gt;

&lt;p&gt;본 포스트에서는 Baysian Nerural Networks에 대해서 기초부터 최신 논문 동향까지를 기술한다.&lt;/p&gt;

&lt;p&gt;먼저 기본적인 설명은 &lt;a href=&quot;https://medium.com/neuralspace/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2&quot;&gt;Bayesian Neural Network&lt;/a&gt; post를 참고하여 재구성하였다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/neuralspace/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2&quot;&gt;Bayesian Neural Network&lt;/a&gt; postsms 8개의 post를 통해서 설명하고 있다. 이 중 본 post는 첫 번째인 Need for Bayesian Neural Networks &lt;a href=&quot;https://medium.com/neuralspace/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2&quot;&gt;Bayesian Neural Network&lt;/a&gt;를 주로 참조하였다.&lt;/p&gt;

&lt;p&gt;Baysian Nerural Networks를 이해하기 위해 먼저 point-estimate에 대해서 이해할 필요가 있다.
Point-estimate : 아래 그림의 왼쪽 네트워크 예시와 같이 weight(filter의 각 element)의 가 sinle point로 표현되는 것을 의미한다.
Baysian Nerural Network: Point-estimate와는 다르게 아래의 그림에서 오른쪽 네트워크와 같이 weight가 확률의 형태로 표기됨을 의미한다.&lt;/p&gt;

&lt;p&gt;그럼 Point-estimate는 왜 문제인가? Over-fitting 때문&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;결국 최종단에서 softmax가 각 pixel 또는, 이미지들의 class를 결정하는데 있어, 하나의 class외에 나머지를 squish하여 zero 근처로 보내버리고 오직 하나만을 maximize하는 방향으로 가버리는데 이는 one class에 대한 overconfident 결정일 수 있다. 특히 imbalanced dataset에서 이러한 overconfident 결정은 더 두드러지게 나타난다.
 즉, 동물을 classfication하는 문제라고하면 개일 확률 0.9로 만들고 나머지는 0.1 이하로 만드는 결정을 할 수 있다.
 때로는 강아지 0.4 늑대 0.3 고양이 0.2로 inference하는 방식이 네트워크의 overfit를 방지할 수 있다.
 -&amp;gt; 그럼 뒤에 결정은 어떻게 하는가? -&amp;gt; 좀 더 고민해 보자&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*n6Td0BSmvCGaTYaIJEqF-g.png&quot; alt=&quot;Bayesian_Net&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그런데 Overfitting or making overconfident decisions문제를 풀기위해 많은 regularization techniques들이 존재한다.
이들과 Baysian Nerural Network의 차이는 무엇인가? -&amp;gt; 기존 regularization techniques은 결정된 정보의 불확실함을 표현하지 못한다.
그러나 Baysian Nerural Network는 결정된 부분에서 불확실함이 네트워크에 표현이 가능하다.&lt;/p&gt;

&lt;h2 id=&quot;the-practicality-of-bayesian-neural-networks&quot;&gt;The practicality of Bayesian neural networks&lt;/h2&gt;
&lt;p&gt;이 파트는 Bayesian neural networks를 어떻게 구현할 것인가에 대한 얘기로, 네트워크의 여러 파라미터 (weights, activation results등) 중에서 무엇을 확률 모델로 바꿀것 인가를 다루는 문제이다. 가장 쉽게 생각할 수 있는건 weights를 확률 모델로 가져가는 것이다. 그러나 여태까지 그 누구도 weights를 distribution 모델로 가져가서 성공한 사례는 없다. 그 이유는 너무나 많은 weight 수와 모델의 크기 때문이 아닐까라고 예측되고있다.&lt;a href=&quot;https://medium.com/neuralspace/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2&quot;&gt;1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;그러한 이유로 Bayesian neural networks 연구를 시간의 순서대로 살펴볼 필요가 있다. 초기 연구는 가능성을 보기 위해 FC 구조의 neural network에서 model posterior에 대해 근사화하는 것을 시작으로 한다. 초기에는 posterior 확률로 가우시안 분포와 같은 simple variational distribution을 사용하였고, 네트워크가 학습을 통해서 이러한 true posterior probability에 가까운 분포를 만들어 내도록 학습되었다. 이를 위해 확률 분포간의 가격을 최소화하는 KL-divergence가 사용되었다.&lt;/p&gt;

&lt;p&gt;그러나 이 연구의 문제는 gaussian 근사화로 인해 model parameter들의 수 증가가 너무나 커져서 계산적으로 비싸다는 것이다. 예로 가우시안 근사화에 사용된 파라미터만으로 모델 파라미터들의 두배가 된다는 것이다.&lt;/p&gt;

&lt;p&gt;-» 검토 필요 : 그런데 dropout을 사용한 전통적인 방식이 동일한 predictive performance를 보여준다는 결과가 나왔다???&lt;/p&gt;

&lt;p&gt;이는 모델의 모든 파라미터들은 변수가 될 수 있고, 이 각각의 변수들을 bayesian 확률 모델로 정의하는 것은 엄청난 계산 복잡도를 가져온다는 것이다. 그래서 Bayesian neural networks 모델을 만드는 수 많은 방법들이 존재하는데 이 포스트에서는 backpropagation에 bayes 이론을 적용하는 것을 집중적으로 살펴본다.&lt;/p&gt;

&lt;h2 id=&quot;bayes-by-backprop&quot;&gt;Bayes by Backprop&lt;/h2&gt;

&lt;p&gt;Bayes by backprop는 &lt;a href=&quot;https://arxiv.org/abs/1505.05424&quot;&gt;Blundell, et al.&lt;/a&gt;에 의해서 처음 도입되었다. 이 논문에서는 gradient의 unbiased Monte Carlo estimates를 사용한 앙상블들로 네트워크의 파라미터들을 학습시켰다.&lt;/p&gt;

&lt;p&gt;그런데 네트워크 파라미터가 점점 증가함에 따라 정확하게 학습시키는 것은 불가능하였다. 
이에 대한 해결책으로 제시된 방법은 최근에 사용되는 variational approximation 기술이다.
기존 Monte Carlo 기술은 너무나 많은 앙상블이 요구되기에 Bayesian posterior distribution을 approximate시키는 것은 정말 어렵기 때문이다.&lt;/p&gt;

&lt;p&gt;왜 학습이 어려운지는 &lt;a href=&quot;https://medium.com/neuralspace/bayesian-neural-network-series-post-2-background-knowledge-fdec6ac62d43&quot;&gt;Bayesian neural networks&lt;/a&gt; 2번 째 blog post에 다음과 같이 잘 설명되어있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bayes theorem은 다음과 같다.
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*7iOrI5jb6Dae630hCYENjA.png&quot; alt=&quot;Bayes theorem&quot; /&gt;
일반적으로 supervised NN은 data x가 주어질 경우 model parameters θ를 찾는 문제로, 이로부터 posterior probability P(θ|x)가 정의된다.
여기서 P(θ) 는 prior이며, P(x|θ) which is the likelihood로 data distribution을 갖는다. 
여기서 P(x)는 evidence로 모델로 부터 생성된다. 즉 다음과 같이 얻어진다.
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*FzrX_7Qb7m1n6eXO2zrE9Q.png&quot; alt=&quot;p_x_evidence&quot; /&gt; 
여기서 알 수 있듯이 p(x)를 얻기위해서는 모든 모델 파라미터와 x가 결합되었을떄 발생하는 모든 경우의수를 얻어야하는데, 이는 모델 파라미터의 엄청난 수로 인해 불가능하다. 따라서 이를 approximate하는 방식이 필요한데, 요새는 variational inference라는 모델이 사용된다.
이 방법외에 (Markov Chain Monte Carlo and Monte Carlo Dropout.)방식도 사용된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variational-inference&quot;&gt;Variational Inference&lt;/h2&gt;
&lt;p&gt;variation inference 방식으로 풀기 위해 각 probabilty는 density function을 갖으며, 이를 예측하는 문제라고 가정한다. 따라서 알려진 distribution중에 하나를 target density function으로 가정하는 것으로부터 시작한다. 이로부터 우리가 찾는 문제는 posterior probability와 true distrbition간의 거리를 최대한 가깝게 만드는 것을 목표로한다. 이를 위해서 사용되는 기술이 the Kullback-Liebler (KL) divergence이다.&lt;/p&gt;

&lt;p&gt;예로 모델의 웨이트 파라미터를 w, Data를 D라하면 true posterior probability를 P(w|D)라 할수 있으며, 다른 distribution은 q(w|D)라 할 수 있다.
이때 KL divergence는 다음과 같이 정의된다.
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*b08FgIvbikjpX0ZTraY1sg.png&quot; alt=&quot;KL_divergence&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 최적화 문제를 풀어보면 다음과 같이 전개되는데 ingegral function으로 인해 사실상 푸는 것이 불가능하다.
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*sZGFVuHKPZdhROYNWEy9YQ.png&quot; alt=&quot;KL_divergence2&quot; /&gt;
식에서 보면 알 수 있듯이 true posterior function p(w|D)는 사실상 다루기 어렵기 때문에 q(W|D)를 근사화하여 문제를 푸는 것이 훨씬 용이하다.&lt;/p&gt;

&lt;p&gt;이에 다음과 같이 모델을 근사화 할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*88qCMa1S_2v-dWSbtwEG_A.png&quot; alt=&quot;KL_divergence3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 이 모델은 train이 가능한 형태이다
왜??? -&amp;gt; 이해하기 매우어렵지만 좀더 진행후 이해해보고자 한다.&lt;/p&gt;

&lt;p&gt;그러나 이해하기 앞서 수식에서 보이는 것처럼 weight의 수에 영향을 받기 때문에 weight pruning 기술을 적용하여 networkdml sparsity를 감소시켜 네트워크의 성능을 감소시키지 않는 선에서 모델의 파라미터의 수를 감소시킬 필요가 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*lTZBJeYsohUk7RaFrgg1Jg.png&quot; alt=&quot;Bayesian_inference_total_form&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*n7hGf0h9Q-nwyUex1889Zg.png&quot; alt=&quot;bayes_theorem&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 09 Mar 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019/03/09/Bayesian-Neural-Net.html</link>
        <guid isPermaLink="true">http://localhost:4000/2019/03/09/Bayesian-Neural-Net.html</guid>
        
        
      </item>
    
      <item>
        <title>Pr Gcn 01</title>
        <description>&lt;p&gt;Title :&lt;/p&gt;
</description>
        <pubDate>Thu, 27 Dec 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018/12/27/PR-GCN-01.html</link>
        <guid isPermaLink="true">http://localhost:4000/2018/12/27/PR-GCN-01.html</guid>
        
        
      </item>
    
      <item>
        <title>Natural Language Processing study</title>
        <description>&lt;p&gt;최근 natural language 공부를 시작하였습니다.&lt;/p&gt;

&lt;p&gt;본 페이지에서는 GPU의 대표 모델별 architecture와 CUDA 프로그래밍&lt;/p&gt;

&lt;p&gt;GPU에 대해서 이해를 위해선 먼저 엔비디아의 주요 제품군을 살펴볼 필요가 있다.
크게 그래픽카드 칩셋인 지포스(GeForce) 시리즈와 컴퓨터 그래픽스 개발를 위해 만들어진
그래픽 카드 칩셋 쿼드로(Quadro) 시리즈, 그리고 딥러닝과 같은 고성능 컴퓨팅용
카드인 테슬라(Tesla) 시리즈가 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://img.danawa.com/images/descFiles/4/520/3519715_1514702757296.png&quot; alt=&quot;GPU 구조1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 중 고성능 컴퓨팅 카드인 테슬라 시리즈 분류의 GPU 구조에 대해서 설명한다. 
위의 그림과 같이 CPU 대비 GUP의 가장 큰 특징은 다수의 ALU로 구성되어 있다는 것인데,
이는 동시에 수많은 ㄴ벡터연산을 하드웨어 수행이 가능하다는것이다. 이러한 특징이 딥러닝 연산에 최적화되어
있는 것이다.
즉, CPU는 ALU 보다는 Controlblock과 cache가 많은 부분 구성되어 있었으나, ALU 연산에는 과도한
SPEC이기에 이러한 요소를 줄이고 ALU 연산에 최적화 시킨 것이다.&lt;/p&gt;

&lt;p&gt;테슬라 시리즈에서 세대별 모델의 순서는 Fermi, kepler, Maxwell, Pascal로 아래의 두 그림과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/J3b8uJn.png&quot; alt=&quot;GPU 구조2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://drmola.com/files/attach/images/56517/096/049/a66371d08d953fa9af604a66c97cbf3e.png&quot; alt=&quot;GPU 구조3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 하드웨어 구조에서 최적의 컴퓨팅 연산을 위해선 ALU 로직과 메모리 접근,fetcing 그리고 재분배 등과 같은
다양한 메모리 제어 방법이 필요하다 이에 따라 이러한 하드웨어와 최적으로 운용하기위한 SW가 필요하며 이러한
기술중 엔디비아가 개발한 대표적인 SW가 CUDA이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/aOKAnQn.png&quot; alt=&quot;GPU 구조4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/CHw8FjP.png&quot; alt=&quot;GPU 구조5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 GPU 구조에서 CUDA 프로그래밍이 하는 역할은 다음 그림들과 같이 순차적으로 발생한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://cfile26.uf.tistory.com/image/221F474C58114C801BED34&quot; alt=&quot;CUDA 1&quot; /&gt;
&lt;img src=&quot;http://cfile27.uf.tistory.com/image/211A474C58114C811E95DB&quot; alt=&quot;CUDA 2&quot; /&gt;
&lt;img src=&quot;http://cfile9.uf.tistory.com/image/2223824C58114C82177403&quot; alt=&quot;CUDA 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CUDNN은 딥러닝을 위해서 반복적으로 사용되는 컨볼루션, 폴링, signoid, Batchnorm(?)과 같은 기본적인 기능들을 정해진 알고리즘에 따라서 cuda 프로그래밍으로 사전에 구현해 놓은 라이브러리를 가리킨다. 예로 컨볼루션의 경우 gemm, fast gemm, FFT, Winograd 알고리즘이 대표적인 것이다.
아래의 그림은 gemm을 CUDA 프로그래밍으로 구현하는 기본 메카니즘을 보여준다.
&lt;img src=&quot;https://www.groundai.com/media/arxiv_projects/11346/genr.svg&quot; alt=&quot;CuDNN&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 16 Dec 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018/12/16/NLP_DL_lab.html</link>
        <guid isPermaLink="true">http://localhost:4000/2018/12/16/NLP_DL_lab.html</guid>
        
        
      </item>
    
      <item>
        <title>cs231n review (course 1-4)</title>
        <description>&lt;p&gt;본 페이지는 shell script를 연습하기 위한 페이지로 기억의 끝자락을 따라서 그저 끄적여 본다.&lt;/p&gt;

&lt;p&gt;source config/server.config&lt;/p&gt;

&lt;p&gt;선언되어야할 변수들&lt;/p&gt;

&lt;p&gt;$aa_preprocessor&lt;/p&gt;

&lt;p&gt;$hci_file&lt;/p&gt;

&lt;p&gt;$png_file&lt;/p&gt;

&lt;p&gt;AA preprocessor container 수행&lt;/p&gt;

&lt;p&gt;docker start $aa_preprocessor      –&amp;gt; docker container 시작&lt;/p&gt;

&lt;p&gt;docker exec $aa_preprocessor sh precoess.sh [-i] $hci_file [-o] $png_file
  -&amp;gt; 이안에서 hci2png conversion이 일어남&lt;/p&gt;

&lt;p&gt;각 container 수행&lt;/p&gt;

</description>
        <pubDate>Tue, 15 May 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018/05/15/shell-practice.html</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/15/shell-practice.html</guid>
        
        
      </item>
    
      <item>
        <title>PR review (title: learning to cluster for proposal-free instance segmentation)</title>
        <description>
</description>
        <pubDate>Tue, 15 May 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018/05/15/PR-RI-01.html</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/15/PR-RI-01.html</guid>
        
        
      </item>
    
      <item>
        <title>cs231n review (course 1-4)</title>
        <description>&lt;p&gt;본 페이지에서는 스탠포드 대학의 ccs231n 코스를 지극히 개인관점에서 리뷰 및 핵심내용을 요약하고자 한다.&lt;/p&gt;

&lt;p&gt;먼저 1-4 class의 내용을 다음과 같이 요약한다.&lt;/p&gt;
</description>
        <pubDate>Mon, 14 May 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018/05/14/cs231n-review.html</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/14/cs231n-review.html</guid>
        
        
      </item>
    
      <item>
        <title>GPU architecture and CUDA programming</title>
        <description>&lt;p&gt;본 페이지에서는 GPU의 대표 모델별 architecture와 CUDA 프로그래밍&lt;/p&gt;

&lt;p&gt;GPU에 대해서 이해를 위해선 먼저 엔비디아의 주요 제품군을 살펴볼 필요가 있다.
크게 그래픽카드 칩셋인 지포스(GeForce) 시리즈와 컴퓨터 그래픽스 개발를 위해 만들어진
그래픽 카드 칩셋 쿼드로(Quadro) 시리즈, 그리고 딥러닝과 같은 고성능 컴퓨팅용
카드인 테슬라(Tesla) 시리즈가 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://img.danawa.com/images/descFiles/4/520/3519715_1514702757296.png&quot; alt=&quot;GPU 구조1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 중 고성능 컴퓨팅 카드인 테슬라 시리즈 분류의 GPU 구조에 대해서 설명한다. 
위의 그림과 같이 CPU 대비 GUP의 가장 큰 특징은 다수의 ALU로 구성되어 있다는 것인데,
이는 동시에 수많은 ㄴ벡터연산을 하드웨어 수행이 가능하다는것이다. 이러한 특징이 딥러닝 연산에 최적화되어
있는 것이다.
즉, CPU는 ALU 보다는 Controlblock과 cache가 많은 부분 구성되어 있었으나, ALU 연산에는 과도한
SPEC이기에 이러한 요소를 줄이고 ALU 연산에 최적화 시킨 것이다.&lt;/p&gt;

&lt;p&gt;테슬라 시리즈에서 세대별 모델의 순서는 Fermi, kepler, Maxwell, Pascal로 아래의 두 그림과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/J3b8uJn.png&quot; alt=&quot;GPU 구조2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://drmola.com/files/attach/images/56517/096/049/a66371d08d953fa9af604a66c97cbf3e.png&quot; alt=&quot;GPU 구조3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 하드웨어 구조에서 최적의 컴퓨팅 연산을 위해선 ALU 로직과 메모리 접근,fetcing 그리고 재분배 등과 같은
다양한 메모리 제어 방법이 필요하다 이에 따라 이러한 하드웨어와 최적으로 운용하기위한 SW가 필요하며 이러한
기술중 엔디비아가 개발한 대표적인 SW가 CUDA이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/aOKAnQn.png&quot; alt=&quot;GPU 구조4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/CHw8FjP.png&quot; alt=&quot;GPU 구조5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 GPU 구조에서 CUDA 프로그래밍이 하는 역할은 다음 그림들과 같이 순차적으로 발생한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://cfile26.uf.tistory.com/image/221F474C58114C801BED34&quot; alt=&quot;CUDA 1&quot; /&gt;
&lt;img src=&quot;http://cfile27.uf.tistory.com/image/211A474C58114C811E95DB&quot; alt=&quot;CUDA 2&quot; /&gt;
&lt;img src=&quot;http://cfile9.uf.tistory.com/image/2223824C58114C82177403&quot; alt=&quot;CUDA 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CUDNN은 딥러닝을 위해서 반복적으로 사용되는 컨볼루션, 폴링, signoid, Batchnorm(?)과 같은 기본적인 기능들을 정해진 알고리즘에 따라서 cuda 프로그래밍으로 사전에 구현해 놓은 라이브러리를 가리킨다. 예로 컨볼루션의 경우 gemm, fast gemm, FFT, Winograd 알고리즘이 대표적인 것이다.
아래의 그림은 gemm을 CUDA 프로그래밍으로 구현하는 기본 메카니즘을 보여준다.
&lt;img src=&quot;https://www.groundai.com/media/arxiv_projects/11346/genr.svg&quot; alt=&quot;CuDNN&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 13 May 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018/05/13/gpu-cuda.html</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/13/gpu-cuda.html</guid>
        
        
      </item>
    
      <item>
        <title>GPU architecture and CUDA programming</title>
        <description>&lt;p&gt;본 페이지에서는 “ 머신러닝과 블록체인을 떠받치는 GPU의 모든기술”이라는  도서를 Deep Dive 하여 모두 파헤치기 
위해 작성하였다.&lt;/p&gt;

&lt;p&gt;먼저 GPU의 주된 처리 방식에 대해서 정리해본다.&lt;/p&gt;

&lt;p&gt;저자의 책  그림 1.13 보면 알 수 있듯이, GPU와 CPU는 일반적으로 DMA(Direct Memory Access)엔진 을 
사용하여  양쪽 메모리 사이의  데이터를 전송하였다. ( 물리적 연결은 어떻게? – DMA에 대하여 대하여 보다 ㄱ오부할 것)&lt;/p&gt;

&lt;p&gt;메모리 공유방법 :
1) DMA
2) PCI Express를 경유 -&amp;gt; 통신 오버헤드가 큼(gpu -&amp;gt;PCI -&amp;gt;보드-&amp;gt;CPU 등등)&lt;/p&gt;

&lt;p&gt;이 방식들의 문제는 무엇인가?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;GPU와 CPU간에 별도 메모리가 존재하면 깊은 복사로 인해 오버헤드가 상당하다. 따라서 공통의 메모리 공간을 갖추는 것이 중요하나 
다음의 한계로 어렵다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1) CPU - 대용량 메모리 필요
2) GPU - 고연산 성능을 가능하기 위한 고 대역폭 메모리 필요&lt;/p&gt;

&lt;p&gt;-&amp;gt; 즉 gpu 제조사는 이러한 한계속에서 공통 메모리 공간을 다루는 것이 중요한 개발 목표가 됨 -&amp;gt; cpu 제조사도 마찬가지인듯?
예로 CPU 제조사에서 고 대역폭 요구사항을 맞춰 주기 위하여 일반적으로 cpu에서 사용되는 DDR3/4 메모리 경우 대역폭이 부족하여 GDDR DRAM등의 방법으로 메모리 대역폭을 개선중에 있다. 또한 L4 캐시를 장착하여 GPU가 요구하는 높은 메모리 대역을 실현하고 있다.zz&lt;/p&gt;

&lt;p&gt;CUDNN은 딥러닝을 위해서 반복적으로 사용되는 컨볼루션, 폴링, signoid, Batchnorm(?)과 같은 기본적인 기능들을 정해진 알고리즘에 따라서 cuda 프로그래밍으로 사전에 구현해 놓은 라이브러리를 가리킨다. 예로 컨볼루션의 경우 gemm, fast gemm, FFT, Winograd 알고리즘이 대표적인 것이다.
아래의 그림은 gemm을 CUDA 프로그래밍으로 구현하는 기본 메카니즘을 보여준다.
&lt;img src=&quot;https://www.groundai.com/media/arxiv_projects/11346/genr.svg&quot; alt=&quot;CuDNN&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 18 Feb 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018/02/18/gpu-cuda_(2)_Deep_Dive_into_GPU_Books.html</link>
        <guid isPermaLink="true">http://localhost:4000/2018/02/18/gpu-cuda_(2)_Deep_Dive_into_GPU_Books.html</guid>
        
        
      </item>
    
  </channel>
</rss>
